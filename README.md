### Song Recommendation Based on Facial Expression
This project combines music and facial expression analysis to create a personalized experience. Using the DEAM dataset, we apply machine learning to predict emotions from audio and facial features, developing a recommendation engine that syncs songs with the user's emotions for an immersive experience.

#### Music Emotion Recognition (MER) 
Analyzing emotions from pitch, frequency, and amplitude in audio involves understanding
how these features contribute to the overall emotional tone of music. The pitch, whether high
or low, can convey feelings of excitement or calmness. Frequency distribution, spanning from
low bass to high treble, influences the emotional depth and brightness of a piece. Amplitude,
indicating loudness or softness, contributes to the intensity or subtlety perceived in the music.
By examining changes and patterns in these features, one can discern the emotional dynamics
of a song. Integrating machine learning techniques and training models on annotated
emotional data enables the extraction of emotional cues from audio signals.
Our dataset has 4 target values for each song, the mean_valence, mean_arousal, std_arousal,
and std_valence.

Valence refers to the pleasantness or unpleasantness of an emotion. Emotions can be classified
on a scale from positive to negative valence. For example, joy and love are considered positive
valence emotions, while anger and sadness are negative valence emotions.
Arousal refers to the level of physiological activation or intensity associated with an emotion.
Emotions can range from low arousal (calm or relaxed) to high arousal (excited or agitated).
For instance, calmness and contentment are low-arousal emotions, while fear and excitement
are high-arousal emotions.

<img src="https://github.com/user-attachments/assets/c96faac7-de1a-4242-9416-132025b76ddc" width="100" height="100"/>




